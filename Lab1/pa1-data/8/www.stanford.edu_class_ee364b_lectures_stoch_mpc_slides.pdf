stochastic model predictive control page 1 stochastic model predictive control stochastic finite horizon control stochastic dynamic programming certainty equivalent model predictive control prof s boyd ee364b stanford university page 2 causal state feedback control linear dynamical system over finite time horizon xt 1 axt but wt t 0 t 1 xt r n is state ut r m is the input at time t wt is the process noise or exogeneous input at time t xt x0 xt is the state history up to time t causal state feedback control ut t xt t x0 w0 wt 1 t 0 t 1 t r t 1 n r m called the control policy at time t prof s boyd ee364b stanford university 1 page 3 stochastic finite horizon control x0 w0 wt 1 is a random variable objective j e t 1 t 0 lt xt ut lt xt convex stage cost functions lt r n r m r t 0 t 1 convex terminal cost function lt r n r j depends on control policies 0 t 1 constraints ut ut t 0 t 1 convex input constraint sets u0 ut 1 stochastic control problem choose control policies 0 t 1 to minimize j subject to constraints prof s boyd ee364b stanford university 2 page 4 stochastic finite horizon control an infinite dimensional problem variables are functions 0 t 1 can restrict policies to finite dimensional subspace eg t all affine key idea we have recourse aka feedback closed loop control we can change ut based on the observed state history x0 xt cf standard open loop optimal control problem where we commit to u0 ut 1 ahead of time in general case need to evaluate j for given control policies via monte carlo simulation prof s boyd ee364b stanford university 3 page 5 solution via dynamic programming let vt xt be optimal value of objective from t on starting from initial state history xt vt xt lt xt j ev0 x0 vt can be found by backward recursion for t t 1 0 vt xt inf v u lt xt v e vt 1 xt axt bv wt xt vt t 0 t are convex functions optimal policy is causal state feedback  t xt argmin v u lt xt v e vt 1 xt axt bv wt xt prof s boyd ee364b stanford university 4 page 6 independent process noise assume x0 w0 wt 1 are independent vt depends only on the current state xt and not the state history xt bellman equations vt xt lt xt for t t 1 0 vt xt inf v u lt xt v evt 1 axt bv wt optimal policy is a function of current state xt  xt argmin v u lt xt v evt 1 axt bv wt prof s boyd ee364b stanford university 5 page 7 linear quadratic stochastic control special case of linear stochastic control ut r m x0 w0 wt 1 are independent with ex0 0 ewt 0 ex0x t 0  ewtw t t wt lt xt ut xt t qtxt ut t rtut with qt 0 rt 0 lt xt xt t qt xt with qt 0 prof s boyd ee364b stanford university 6 page 8 can show value functions are quadratic ie vt xt x t t ptxt qt t 0 t bellman recursion pt qt qt 0 for t t 1 0 vt z inf v zt qtz v t rtv e az bv wt t pt 1 az bv wt qt 1 works out to pt a t pt 1a a t pt 1b b t pt 1b rt 1 b t pt 1a qt qt qt 1 tr wtpt 1 prof s boyd ee364b stanford university 7 page 9 optimal policy is linear state feedback  t xt ktxt kt b t pt 1b rt 1 b t pt 1a which strangely does not depend on  w0 wt 1 optimal cost j ev0 x0 tr p0 q0 tr p0 t 1 t 0 tr wtpt 1 prof s boyd ee364b stanford university 8 page 10 certainty equivalent model predictive control at every time t we solve the certainty equivalent problem minimize t 1  t lt x u lt xt subject to u u  t t 1 x 1 ax bu w t  t t 1 with variables xt 1 xt ut ut 1 and data xt wt t wt 1 t wt t wt 1 t are predicted values of wt wt 1 based on xt eg conditional expectations call solution xt 1 xt ut ut 1 we take mpc xt ut  mpc is a function of xt since wt t wt 1 t are functions of xt prof s boyd ee364b stanford university 9 page 11 certainty equivalent model predictive control widely used eg in revenue management based on bad approximations future values of disturbance are exactly as predicted there is no future uncertainty in future no recourse is available yet often works very well prof s boyd ee364b stanford university 10 page 12 example system with n 3 states m 2 inputs horizon t 50 a b chosen randomly quadratic stage cost lt x u x 2 2 u 2 2 quadratic final cost lt x x 2 2 constraint set u u u 0.5 x0 w0 wt 1 iid n 0 0.25 i prof s boyd ee364b stanford university 11 page 13 stochastic mpc sample trajectory sample trace of x1 and u1 0 10 20 30 40 50 2 1 0 1 2 0 10 20 30 40 50 0.5 0 0.5 x 1 t u 1 t t prof s boyd ee364b stanford university 12 page 14 cost histogram 0 200 400 600 800 1000 0 50 100 150 0 200 400 600 800 1000 0 50 100 150 j mpc j relax j relax j sat prof s boyd ee364b stanford university 13 page 15 simple lower bound for quadratic stochastic control x0 w0 wt 1 independent quadratic stage and final cost relaxation ignore ut yields linear quadratic stochastic control problem solve relaxed problem exactly optimal cost is j relax j jrelax for our numerical example j mpc 224.7 via monte carlo j sat 271.5 linear quadratic stochastic control with saturation j relax 141.3 prof s boyd ee364b stanford university 14
