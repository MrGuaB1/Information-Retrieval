cs 294 5 statistical natural language processing cs 221 artificial intelligence lecture 6 advanced machine learning sebastian thrun and peter norvig slide credit mark pollefeys dan klein chris manning outline clustering k means em spectral clustering dimensionality reduction 2 the unsupervised learning problem 3 many data points no labels unsupervised learning google street view 4 k means 5 many data points no labels k means choose a fixed number of clusters choose cluster centers and point cluster allocations to minimize error can t do this by exhaustive search because there are too many possible allocations algorithm fix cluster centers allocate points to closest cluster fix allocation compute best cluster centers x could be any set of features for which we can compute a distance careful about scaling from marc pollefeys comp 256 2003 k means k means from marc pollefeys comp 256 2003 k means clustering using intensity alone and color alone image clusters on intensity clusters on color from marc pollefeys comp 256 2003 results of k means clustering k means is an approximation to em model hypothesis space mixture of n gaussians latent variables correspondence of data and gaussians we notice given the mixture model it's easy to calculate the correspondence given the correspondence it's easy to estimate the mixture models expectation maximzation idea data generated from mixture of gaussians latent variables correspondence between data items and gaussians generalized k means em gaussians 13 ml fitting gaussians 14 learning a gaussian mixture with known covariance m step e step expectation maximization converges proof neal hinton mclachlan krishnan e m step does not decrease data likelihood converges at local minimum or saddle point but subject to local minima em clustering results http www ece neu edu groups rpl kmeans practical em number of clusters unknown suffers badly from local minima algorithm start new cluster center if many points unexplained kill cluster center that doesn t contribute use aic bic criterion for all this if you want to be formal 18 spectral clustering 19 spectral clustering 20 the two spiral problem 21 spectral clustering overview data similarities block detection slides from dan klein sep kamvar chris manning natural language group stanford university eigenvectors and blocks block matrices have block eigenvectors near block matrices have near block eigenvectors ng et al nips 02 1 1 0 0 1 1 0 0 0 0 1 1 0 0 1 1 eigensolver 71 71 0 0 0 0 71 71 l 1 2 l 2 2 l 3 0 l 4 0 1 1 2 0 1 1 0 2 2 0 1 1 0 2 1 1 eigensolver 71 69 14 0 0 14 69 71 l 1 2.02 l 2 2.02 l 3 0.02 l 4 0.02 slides from dan klein sep kamvar chris manning natural language group stanford university spectral space can put items into blocks by eigenvectors resulting clusters independent of row ordering 1 1 2 0 1 1 0 2 2 0 1 1 0 2 1 1 71 69 14 0 0 14 69 71 e 1 e 2 e 1 e 2 1 2 1 0 2 1 0 1 1 0 1 2 0 1 2 1 71 14 69 0 0 69 14 71 e 1 e 2 e 1 e 2 slides from dan klein sep kamvar chris manning natural language group stanford university the spectral advantage the key advantage of spectral clustering is the spectral space representation slides from dan klein sep kamvar chris manning natural language group stanford university measuring affinity intensity texture distance from marc pollefeys comp 256 2003 scale affects affinity from marc pollefeys comp 256 2003 from marc pollefeys comp 256 2003 dimensionality reduction 29 the space of digits in 2d 30 m brand merl dimensionality reduction with pca 31 gaelvaroquaux normalesup org linear principal components fit multivariate gaussian compute eigenvectors of covariance project onto eigenvectors with largest eigenvalues 32 other examples of unsupervised learning 33 mean face after alignment slide credit santiago serrano eigenfaces 34 slide credit santiago serrano non linear techniques isomap local linear embedding 35 isomap science m balasubmaranian and e schwartz scape drago anguelov et al 36
