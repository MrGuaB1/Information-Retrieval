smoothing cs224n section 2 pa2 & em shrey gupta january 21 2011 outline for today interactive session brief review of mt examples brief em review statistical machine translation p e f p f e p e p f max e p e f max e p f e p e language models p e help alleviate shortcomings of p f e concepts translation probabilities t distortion probabilities d fertility  null pa2 requirements naive model ibm model 1 ibm model 2 integration with decoder ibm model 1 simplest of the ibm models does not consider word order bag of words approach does not model one to many alignments computationally inexpensive useful for parameter estimations that are passed on to more elaborate models ibm model 1 we only learn the translation probabilities ibm model 1 steps initialize the probabilities uniformly e step m step calculate repeat until convergence let's do an example ibm model 2 in model two we learn translation probabilities and also distortion probabilities ibm model 2 ibm model 2 tries to learn the alignment probabilities in addition to the translation probabilities the alignment probabilities are handled at an abstract level by grouping alignment pairs into buckets let the number of buckets be n indexed from 0 n 1 for a pair let n the pair is placed is bucket n if n n 1 or in the n th bucket if n n ibm model 2 in model 2 during the em step we also collect fractional counts of each bucket and subsequently normalize the same to have a true probability distribution many possible implementations variable number of buckets signed buckets hand fixed weights em revisited similar to k means soft count v s hard counts http home dei polimi it matteucc clustering tutorial_html appletkm html http lcn epfl ch tutorial english gaussian html index html tips start early read knight's tutorial plan your approach before you start questions
