kl divergence or relative entropy page 1 kl divergence or relative entropy two pmfs p x and q x d pq x x p x log p x q x 5 say 0 log 0 q 0 otherwise p log p 0 d pq ep log p x q x 6 i x y d p x y p x p y 7 12 page 2 measure of how different two proba bility distributions are the average number of bits that are wasted by encoding events from a distribution p with a code based on a not quite right distribution q d pq 0 d pq 0 iff p q not a metric not commutative doesn t satisfy triangle equality 13 page 3 slide on d pq vs d qp 14 page 4 cross entropy entropy uncertainty lower entropy determining efficient codes knowing the structure of the language good measure of model quality entropy measure of surprise how surprised we are when w follows h is pointwise entropy h w h log 2 p w h p w h 1 p w h 0 total surprise htotal n j 1 log 2 m wj w1 w2 wj 1 log 2 m w1 w2 wn 15 page 5 formalizing through cross entropy our model of language is q x how good a model is it idea use d pq where p is the correct model problem we don t know p but we know roughly what it is like from a corpus cross entropy h x q h x d pq 8 x p x log q x ep log 1 q x 9 16 page 6 cross entropy of a language l xi p x according to a model m h l m lim n 1 n x1n p x1n log m x1n if the language is nice h l m lim n 1 n log m x1n 10 ie it's just our average surprise for large n h l m 1 n log m x1n 11 since h l is fixed if unknown minimiz ing cross entropy is equivalent to minimiz ing d pm providing independent test data assume l xi is stationary does t change over time ergodic doesn t get stuck 17 page 7 entropy of english text 27 letter alphabet model cross entropy bits zeroth order 4.76 log 27 first order 4.03 second order 2.8 shannon's experiment 1.3 1.34 18 page 8 perplexity perplexity x1n m 2 h x1n m m x1n 1 n 19
