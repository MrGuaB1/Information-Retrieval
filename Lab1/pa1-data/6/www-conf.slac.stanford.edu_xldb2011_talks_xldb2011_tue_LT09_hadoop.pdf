seamicro sm10000 64 server page 1 does hadoop need one disk per core can shared disks help reduce hadoop cluster cost min xu saisanthosh balakrishnan gary lauterbach sean lie seamicro kshitij sudan seamicro & university of utah page 2 traditional hadoop clusters hadoop workloads large data large data high disk bandwidth server 1 12 disks per 8 cores great server 2 4 disks per 8 cores good server 3 64 disk per 512 cores not good does hadoop really need high bw of one disk per core page 3 page 4 why is 6 the magic number shouldn t the number be more workload dependent profiling individual nodes found server cpus are busy with ctx switches during shuffle and sort high cpu overhead in these ctx switches skewed the results limiting nic interrupt rate significantly reduce the cpu overhead lesson due to all to all data transfers the hadoop shuffle & sort stage has storm of ctx switches 4 page 5 work in progress we continue to do experiments with varying core per disk ratios important to reduce the cpu overhead fix the total of compute nodes and vary the total of disks does hadoop need one disk per core optimized results do not support it results will be workload dependent terasort is an extreme case where no map reduce is done thank you hot chips 2011 5
