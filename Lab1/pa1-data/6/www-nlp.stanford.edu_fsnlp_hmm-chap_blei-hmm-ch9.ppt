powerpoint presentation hidden markov models david meir blei november 1 1999 what is an hmm graphical model circles indicate states arrows indicate probabilistic dependencies between states what is an hmm green circles are hidden states dependent only on the previous state the past is independent of the future given the present what is an hmm purple nodes are observed states dependent only on their corresponding hidden state hmm formalism s k p a b s s 1 s n are the values for the hidden states k k 1 k m are the values for the observations s s s k k k s k s k hmm formalism s k p a b p p i are the initial state probabilities a a ij are the state transition probabilities b b ik are the observation state probabilities a b a a a b b s s s k k k s k s k inference in an hmm compute the probability of a given observation sequence given an observation sequence compute the most likely hidden state sequence given an observation sequence and set of possible models which model most closely fits the data o t o 1 o t o t 1 o t 1 given an observation sequence and a model compute the probability of the observation sequence decoding decoding o t o 1 o t o t 1 o t 1 x 1 x t 1 x t x t x t 1 decoding o t o 1 o t o t 1 o t 1 x 1 x t 1 x t x t x t 1 decoding o t o 1 o t o t 1 o t 1 x 1 x t 1 x t x t x t 1 decoding o t o 1 o t o t 1 o t 1 x 1 x t 1 x t x t x t 1 decoding o t o 1 o t o t 1 o t 1 x 1 x t 1 x t x t x t 1 forward procedure o t o 1 o t o t 1 o t 1 x 1 x t 1 x t x t x t 1 special structure gives us an efficient solution using dynamic programming intuition probability of the first t observations is the same for all possible t 1 length state sequences define o t o 1 o t o t 1 o t 1 x 1 x t 1 x t x t x t 1 forward procedure o t o 1 o t o t 1 o t 1 x 1 x t 1 x t x t x t 1 forward procedure o t o 1 o t o t 1 o t 1 x 1 x t 1 x t x t x t 1 forward procedure o t o 1 o t o t 1 o t 1 x 1 x t 1 x t x t x t 1 forward procedure o t o 1 o t o t 1 o t 1 x 1 x t 1 x t x t x t 1 forward procedure o t o 1 o t o t 1 o t 1 x 1 x t 1 x t x t x t 1 forward procedure o t o 1 o t o t 1 o t 1 x 1 x t 1 x t x t x t 1 forward procedure o t o 1 o t o t 1 o t 1 x 1 x t 1 x t x t x t 1 forward procedure o t o 1 o t o t 1 o t 1 x 1 x t 1 x t x t x t 1 backward procedure probability of the rest of the states given the first state o t o 1 o t o t 1 o t 1 x 1 x t 1 x t x t x t 1 decoding solution forward procedure backward procedure combination o t o 1 o t o t 1 o t 1 best state sequence find the state sequence that best explains the observations viterbi algorithm o t o 1 o t o t 1 o t 1 viterbi algorithm the state sequence which maximizes the probability of seeing the observations to time t 1 landing in state j and seeing the observation at time t x 1 x t 1 j o t o 1 o t o t 1 o t 1 viterbi algorithm recursive computation x 1 x t 1 x t x t 1 o t o 1 o t o t 1 o t 1 viterbi algorithm compute the most likely state sequence by working backwards x 1 x t 1 x t x t 1 x t o t o 1 o t o t 1 o t 1 parameter estimation given an observation sequence find the model that is most likely to produce that sequence no analytic method given a model and observation sequence update the model parameters to better fit the observations a b a a a b b b b o t o 1 o t o t 1 o t 1 parameter estimation a b a a a b b b b probability of traversing an arc probability of being in state i o t o 1 o t o t 1 o t 1 parameter estimation a b a a a b b b b now we can compute the new estimates of the model parameters hmm applications generating parameters for n gram models tagging speech speech recognition o t o 1 o t o t 1 o t 1 the most important thing a b a a a b b b b we can use the special structure of this model to do a lot of neat math and solve problems that are otherwise not solvable
