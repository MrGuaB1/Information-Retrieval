programming and debugging large scale data processing workflows christopher olston yahoo research this talk gives an overview of some work on large scale data processing i have done with my yahoo collaborators the talk begins with overviews of two data processing systems i helped develop pig a dataflow programming environment and hadoop based runtime and nova a workflow manager for pig hadoop the bulk of the talk focuses on debugging and looks at what can be done before during and after execution of a data processing operation pig's automatic example data generator is used before running a pig job to get a feel for what it will do enabling certain kinds of mistakes to be caught early and cheaply the algorithm behind the example generator performs a combination of sampling and synthesis to balance several key factors realism conciseness and completeness of the example data it produces inspector gadget is a framework for creating custom tools that monitor pig job execution we have implemented a dozen user requested tools ranging from data integrity checks to crash cause investigation to performance profiling each in just a few hundreds of lines of code ibis is a system that collects metadata about what happened during data processing for post hoc analysis the metadata is collected from multiple sub systems eg nova pig hadoop that deal with data and processing elements at different granularities eg tables vs records relational operators vs reduce task attempts and offer disparate ways of querying it ibis integrates this metadata and presents a uniform and powerful query interface to users
