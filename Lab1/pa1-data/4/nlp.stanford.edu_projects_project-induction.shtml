the stanford nlp natural language processing group the stanford natural language processing group home people teaching research publications software events local the unsupervised language learning project page overview humans are able to acquire linguistic knowledge in a more or less unsupervised manner although machines lack the contextual situation of a human learner as well as whatever innate knowledge humans might have much of the structure of natural language is distributionally detectable the more linguistic structure that can be automatically learned the less need there is for large marked up corpora which are costly in both time and expertise our primary focus is on grammar induction which aims to find the hierarchical structure of natural language grammar search methods have met with little success and simple distributional approaches that work for part of speech induction do not directly apply for example differentiating noun phrases verb phrases and prepositional phrases requires discovering the three clusters in the left plot which seems easy enough however deciding which sequences are units at all requires telling apart the red and blue clusters on the left which is much harder labeling constituents is easy finding constituents is hard however using a constituent context model which essentially allows distributional clustering in the presence of no overlap constraints we can successfully recover a substantial amount of hierarchical structure even with just a few thousand training sentences our system gives the best published results for unsupervised parsing of the atis corpus and in particular was the first system to beat a baseline of putting right branching over english text measures as unlabeled constituent f 1 see klein and manning 2001 2002 induced trees are substantially better than even the supervised right branching baseline a complementary approach to induction is to focus on relationships between word pairs or dependencies previous work using this approach was quite unsuccessful because it used too simple a dependency model our model borrowing ideas of word classes and valence from supervised parsing dependency models performs well above baseline combining it with the ccm model via a factored model gives extremely good results as shown below incorporation of the dependency model substantially improves performance closing in on the performance achieved by supervised parsers 92.8 for english however interesting issue remain ranging from dealing better with languages like chinese that largely lack morphology and function words to making substantive use of morphology in languages with rich morphology publications trond grenager and christopher d manning 2006 unsupervised discovery of a statistical verb lexicon 2006 conference on empirical methods in natural language processing emnlp 2006 pp 1 8 http nlp stanford edu manning papers verblex pdf ps pdf dan klein and christopher d manning 2004 corpus based induction of syntactic structure models of dependency and constituency proceedings of the 42nd annual meeting of the association for computational linguistics acl 2004 http nlp stanford edu manning papers factored induction camera pdf ps pdf dan klein and christopher d manning 2002 a generative constituent context model for improved grammar induction proceedings of the 40th annual meeting of the association for computational linguistics pp 128 135 http nlp stanford edu manning papers kleinmanningacl2002 pdf ps pdf dan klein and christopher d manning 2002 natural language grammar induction using a constituent context model in thomas g dietterich suzanna becker and zoubin ghahramani eds advances in neural information processing systems 14 nips 2001 cambridge ma mit press vol 1 pp 35 42 http nlp stanford edu manning papers nips gi camera mt pdf ps pdf dan klein and christopher d manning 2001 distributional phrase structure induction proceedings of the fifth conference on natural language learning conll 2001 pp 113 120 http nlp stanford edu manning papers klein_and_manning distributional_phrase_structure_induction conll_2001 pdf ps pdf contact comments about the project page feel free to email chris local links nlp lunch pail lunch nlp reading group javanlp javadocs machines wiki calendar site design by bill maccartney
