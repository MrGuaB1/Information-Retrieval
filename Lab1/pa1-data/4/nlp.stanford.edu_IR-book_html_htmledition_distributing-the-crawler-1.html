distributing the crawler next up previous contents index next dns resolution up crawler architecture previous crawler architecture contents index distributing the crawler we have mentioned that the threads in a crawler could run under different processes each at a different node of a distributed crawling system such distribution is essential for scaling it can also be of use in a geographically distributed crawler system where each node crawls hosts near it partitioning the hosts being crawled amongst the crawler nodes can be done by a hash function or by some more specifically tailored policy for instance we may locate a crawler node in europe to focus on european domains although this is not dependable for several reasons the routes that packets take through the internet do not always reflect geographic proximity and in any case the domain of a host does not always reflect its physical location how do the various nodes of a distributed crawler communicate and share urls the idea is to replicate the flow of figure 20.1 at each node with one essential difference following the url filter we use a host splitter to dispatch each surviving url to the crawler node responsible for the url thus the set of hosts being crawled is partitioned among the nodes this modified flow is shown in figure 20.2 the output of the host splitter goes into the duplicate url eliminator block of each other node in the distributed system the content seen module in the distributed architecture of figure 20.2 is however complicated by several factors unlike the url frontier and the duplicate elimination module document fingerprints shingles cannot be partitioned based on host name there is nothing preventing the same or highly similar content from appearing on different web servers consequently the set of fingerprints shingles must be partitioned across the nodes based on some property of the fingerprint shingle say by taking the fingerprint modulo the number of nodes the result of this locality mismatch is that most content seen tests result in a remote procedure call although it is possible to batch lookup requests there is very little locality in the stream of document fingerprints shingles thus caching popular fingerprints does not help since there are no popular fingerprints documents change over time and so in the context of continuous crawling we must be able to delete their outdated fingerprints shingles from the content seen set s in order to do so it is necessary to save the fingerprint shingle of the document in the url frontier along with the url itself figure 20.2 distributing the basic crawl architecture next up previous contents index next dns resolution up crawler architecture previous crawler architecture contents index 2008 cambridge university press this is an automatically generated page in case of formatting errors you may want to look at the pdf edition of the book 2009 04 07
