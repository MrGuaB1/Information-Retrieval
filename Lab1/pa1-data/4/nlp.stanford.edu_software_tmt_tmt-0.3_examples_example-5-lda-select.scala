stanford tmt example 5 selecting lda model parameters http nlp stanford edu software tmt 0.3 tells scala where to find the tmt classes import scalanlp io _ import scalanlp stage _ import scalanlp stage text _ import scalanlp text tokenize _ import scalanlp pipes pipes global _ import edu stanford nlp tmt stage _ import edu stanford nlp tmt model lda _ import edu stanford nlp tmt model llda _ val source csvfile pubmed oa subset csv idcolumn 1 val tokenizer simpleenglishtokenizer tokenize on space and punctuation casefolder lowercase everything wordsandnumbersonlyfilter ignore non words and non numbers minimumlengthfilter 3 take terms with 3 characters val text source read from the source file column 4 select column containing text tokenizewith tokenizer tokenize with tokenizer above termcounter collect counts needed below termminimumdocumentcountfilter 4 filter terms in 4 docs termdynamicstoplistfilter 30 filter out 30 most common terms documentminimumlengthfilter 5 take only docs with 5 terms set aside 80 percent of the input text as training data val numtrain text data size 4 5 build a training dataset val training ldadataset text take numtrain build a test dataset using term index from the training dataset val testing ldadataset text drop numtrain a list of pairs of number of topics perplexity var scores list empty int double loop over various numbers of topics training and evaluating each model for numtopics list 5 10 15 20 25 val params ldamodelparams numtopics numtopics dataset training val output file lda training signature+ params signature val model traincvb0lda params training output null maxiterations 500 println perplexity computing at numtopics val perplexity model computeperplexity testing println perplexity perplexity at numtopics+ topics perplexity scores numtopics perplexity for numtopics perplexity scores println perplexity perplexity at numtopics+ topics perplexity
