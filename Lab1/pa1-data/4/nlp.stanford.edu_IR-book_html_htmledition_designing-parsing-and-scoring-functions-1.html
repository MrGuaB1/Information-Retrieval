designing parsing and scoring functions next up previous contents index next putting it all together up components of an information previous query term proximity contents index designing parsing and scoring functions common search interfaces particularly for consumer facing search applications on the web tend to mask query operators from the end user the intent is to hide the complexity of these operators from the largely non technical audience for such applications inviting free text queries given such interfaces how should a search equipped with indexes for various retrieval operators treat a query such as rising interest rates more generally given the various factors we have studied that could affect the score of a document how should we combine these features the answer of course depends on the user population the query distribution and the collection of documents typically a query parser is used to translate the user specified keywords into a query with various operators that is executed against the underlying indexes sometimes this execution can entail multiple queries against the underlying indexes for example the query parser may issue a stream of queries run the user generated query string as a phrase query rank them by vector space scoring using as query the vector consisting of the 3 terms rising interest rates if fewer than ten documents contain the phrase rising interest rates run the two 2 term phrase queries rising interest and interest rates rank these using vector space scoring as well if we still have fewer than ten results run the vector space query consisting of the three individual query terms each of these steps if invoked may yield a list of scored documents for each of which we compute a score this score must combine contributions from vector space scoring static quality proximity weighting and potentially other factors particularly since a document may appear in the lists from multiple steps this demands an aggregate scoring function that accumulates evidence of a document's relevance from multiple sources how do we devise a query parser and how do we devise the aggregate scoring function the answer depends on the setting in many enterprise settings we have application builders who make use of a toolkit of available scoring operators along with a query parsing layer with which to manually configure the scoring function as well as the query parser such application builders make use of the available zones metadata and knowledge of typical documents and queries to tune the parsing and scoring in collections whose characteristics change infrequently in an enterprise application significant changes in collection and query characteristics typically happen with infrequent events such as the introduction of new document formats or document management systems or a merger with another company web search on the other hand is faced with a constantly changing document collection with new characteristics being introduced all the time it is also a setting in which the number of scoring factors can run into the hundreds making hand tuned scoring a difficult exercise to address this it is becoming increasingly common to use machine learned scoring extending the ideas we introduced in section 6.1 2 as will be discussed further in section 15.4 1 next up previous contents index next putting it all together up components of an information previous query term proximity contents index 2008 cambridge university press this is an automatically generated page in case of formatting errors you may want to look at the pdf edition of the book 2009 04 07
