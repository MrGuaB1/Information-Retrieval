502 chapter 12 particle based approximate inference algorithm 12.3 likelihood weighting with a data dependent stopping rule pro page 1 502 chapter 12 particle based approximate inference algorithm 12.3 likelihood weighting with a data dependent stopping rule procedure data dependent lw b bayesian network over x z z instantiation of interest u upper bound on cpd entries of z o desired error bound  desired probability of error 1  4 1 o o2 ln 2  2 k z 3 w 0 4 m 0 5 while w uk 6  w lw sample b z z 7 w w w 8 m m 1 9 return w m for this algorithm we can provide a similar theoretical analysis with certain guarantees for this data dependent likelihood weighting approach algorithm 12.3 shows an algorithm that uses data dependent likelihood weighting a data dependent stopping rule to terminate the sampling process when enough weight has been accumulated we can show that theorem 12.1 data dependent lw returns an estimate p for pb z z which with probability at least 1  has a relative error of o we can also place an upper bound on the expected sample size used by the algorithm expected sample size theorem 12.2 the expected number of samples used by data dependent lw is uk pb z  u l k  where  4 1 o o2 ln 2  the intuition behind this result is straightforward the algorithm terminates when w uk the expected contribution of each sample is ieq x w  pb z thus the total number of samples required to achieve a total weight of w uk is m uk pb z although this bound on the expected number of samples is no better than our bound in equation 12.17 the data dependent bound allows us to stop early in cases where we were lucky in our random choice of samples and to continue sampling in cases where we were unlucky 12.2 3.3 ratio likelihood weighting we now move to the problem of computing a conditional probability p y e for a specific event y one obvious approach is ratio likelihood weighting we compute the conditional ratio likelihood weighting
