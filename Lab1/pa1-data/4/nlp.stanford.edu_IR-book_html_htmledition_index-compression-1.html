index compression next up previous contents index next statistical properties of terms up irbook previous references and further reading contents index index compression chapter 1 introduced the dictionary and the inverted index as the central data structures in information retrieval ir in this chapter we employ a number of compression techniques for dictionary and inverted index that are essential for efficient ir systems one benefit of compression is immediately clear we need less disk space as we will see compression ratios of 1 4 are easy to achieve potentially cutting the cost of storing the index by 75 there are two more subtle benefits of compression the first is increased use of caching search systems use some parts of the dictionary and the index much more than others for example if we cache the postings list of a frequently used query term then the computations necessary for responding to the one term query can be entirely done in memory with compression we can fit a lot more information into main memory instead of having to expend a disk seek when processing a query with we instead access its postings list in memory and decompress it as we will see below there are simple and efficient decompression methods so that the penalty of having to decompress the postings list is small as a result we are able to decrease the response time of the ir system substantially because memory is a more expensive resource than disk space increased speed owing to caching rather than decreased space requirements is often the prime motivator for compression the second more subtle advantage of compression is faster transfer of data from disk to memory efficient decompression algorithms run so fast on modern hardware that the total time of transferring a compressed chunk of data from disk and then decompressing it is usually less than transferring the same chunk of data in uncompressed form for instance we can reduce input output i o time by loading a much smaller compressed postings list even when you add on the cost of decompression so in most cases the retrieval system runs faster on compressed postings lists than on uncompressed postings lists if the main goal of compression is to conserve disk space then the speed of compression algorithms is of no concern but for improved cache utilization and faster disk to memory transfer decompression speeds must be high the compression algorithms we discuss in this chapter are highly efficient and can therefore serve all three purposes of index compression in this chapter we define a posting as a docid in a postings list for example the postings list 6 20 45 100 where 6 is the termid of the list's term contains three postings as discussed in section 2.4 2 page postings in most search systems also contain frequency and position information but we will only consider simple docid postings here see section 5.4 for references on compressing frequencies and positions this chapter first gives a statistical characterization of the distribution of the entities we want to compress terms and postings in large collections section 5.1 we then look at compression of the dictionary using the dictionary as a string method and blocked storage section 5.2 section 5.3 describes two techniques for compressing the postings file variable byte encoding and encoding subsections statistical properties of terms in information retrieval heaps law estimating the number of terms zipf's law modeling the distribution of terms dictionary compression dictionary as a string blocked storage postings file compression variable byte codes gamma codes references and further reading next up previous contents index next statistical properties of terms up irbook previous references and further reading contents index 2008 cambridge university press this is an automatically generated page in case of formatting errors you may want to look at the pdf edition of the book 2009 04 07
