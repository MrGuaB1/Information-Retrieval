conditional random fields named entity recognition and the stanford ner software jenny rose finkel stanford university march 9 2007 named entity recognition germany's representative to the european union's veterinary committee werner zwingman said on wednesday consumers should il 2 gene expression and nf kappa b activation through cd28 requires reactive oxygen production by 5 lipoxygenase why ner question answering textual entailment coreference resolution computational semantics ner data bake offs conll 2002 and conll 2003 british newswire multiple languages spanish dutch english german 4 entities person location organization misc muc 6 and muc 7 american newswire 7 entities person location organization time date percent money ace 5 entities location organization person fac gpe bbn penn treebank 22 entities animal cardinal date disease hidden markov models hmms generative find parameters to maximize p x y assumes features are independent when labeling x i future observations are taken into account forward backward maxent markov models memms discriminative find parameters to maximize p y x no longer assume that features are independent do not take future observations into account no forward backward conditional random fields crfs discriminative doesn t assume that features are independent when labeling y i future observations are taken into account the best of both worlds model trade offs global discriminative kinda slow crf local discriminative mid range memm local generative very fast hmm normalization discrim vs generative speed stanford ner crf features are more important than model how to train a new model our features word features current word previous word next word all words within a window orthographic features jenny xxxx il 2 xx prefixes and suffixes jenny j je jen nny ny y label sequences lots of feature conjunctions distributional similarity features large unannotated corpus each word will appear in contexts induce a distribution over contexts cluster words based on how similar their distributions are use cluster ids as features great way to combat sparsity we used alexander clark's distributional similarity code easy to use works great 200 clusters used 100 million words from english gigaword corpus training new models reading data edu stanford nlp sequences documentreaderandwriter interface for specifying input output format edu stanford nlp sequences columndocumentreaderandwriter germany s representative to the european union location o o o o organization organization training new models creating features edu stanford nlp sequences featurefactory interface for extracting features from data makes sense if doing something very different eg chinese ner edu stanford nlp sequences nerfeaturefactory easiest option just add new features here lots of built in stuff computes orthographic features on the fly specifying features edu stanford nlp sequences seqclassifierflags stores global flags initialized from properties file training new models other useful stuff useobservedsequencesonly speeds up training testing makes sense in some applications but not all window how many previous tags do you want to be able to condition on feature pruning remove rare features optimizer lbfgs distributed models trained on conll muc and ace entities person location organization trained on both british and american newswire so robust across both domains models with and without the distributional similarity features incorporating ner into systems ner is a component technology common approach label data pipe output to next stage better approach sample output at each stage pipe sampled output to next stage repeat several times vote for final output sampling ner outputs is fast textual entailment pipeline topological sort of annotators ner parser srl coreference rte parser coreference ne recognizer sr labeler rte sampling example yes arg0 arg1 arg tmp parser coreference ne recognizer sr labeler rte no sampling example yes arg0 arg1 arg loc parser coreference ne recognizer sr labeler rte yes no yes sampling example arg0 arg1 arg tmp parser coreference ne recognizer sr labeler rte yes yes no yes sampling example arg0 arg1 arg2 parser coreference ne recognizer sr labeler rte yes no yes yes sampling example no arg0 arg1 arg tmp parser coreference ne recognizer sr labeler rte yes no yes yes no sampling example parser coreference ne recognizer sr labeler rte yes no yes yes no conclusions ner is a useful technology stanford ner software has pretrained models for english newswire easy to train new models http nlp stanford edu software questions
