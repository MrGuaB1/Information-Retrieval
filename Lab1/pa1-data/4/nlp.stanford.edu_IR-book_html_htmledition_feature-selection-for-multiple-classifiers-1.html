feature selection for multiple classifiers next up previous contents index next comparison of feature selection up feature selection previous frequency based feature selection contents index feature selection for multiple classifiers in an operational system with a large number of classifiers it is desirable to select a single set of features instead of a different one for each classifier one way of doing this is to compute the statistic for an table where the columns are occurrence and nonoccurrence of the term and each row corresponds to one of the classes we can then select the terms with the highest statistic as before more commonly feature selection statistics are first computed separately for each class on the two class classification task versus and then combined one combination method computes a single figure of merit for each feature for example by averaging the values for feature and then selects the features with highest figures of merit another frequently used combination method selects the top features for each of classifiers and then combines these sets into one global feature set classification accuracy often decreases when selecting common features for a system with classifiers as opposed to different sets of size but even if it does the gain in efficiency owing to a common document representation may be worth the loss in accuracy next up previous contents index next comparison of feature selection up feature selection previous frequency based feature selection contents index 2008 cambridge university press this is an automatically generated page in case of formatting errors you may want to look at the pdf edition of the book 2009 04 07
