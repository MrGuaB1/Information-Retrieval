features a crawler should provide next up previous contents index next crawling up overview previous features a crawler must contents index features a crawler should provide distributed the crawler should have the ability to execute in a distributed fashion across multiple machines scalable the crawler architecture should permit scaling up the crawl rate by adding extra machines and bandwidth performance and efficiency the crawl system should make efficient use of various system resources including processor storage and network bandwidth quality given that a significant fraction of all web pages are of poor utility for serving user query needs the crawler should be biased towards fetching useful pages first freshness in many applications the crawler should operate in continuous mode it should obtain fresh copies of previously fetched pages a search engine crawler for instance can thus ensure that the search engine's index contains a fairly current representation of each indexed web page for such continuous crawling a crawler should be able to crawl a page with a frequency that approximates the rate of change of that page extensible crawlers should be designed to be extensible in many ways to cope with new data formats new fetch protocols and so on this demands that the crawler architecture be modular next up previous contents index next crawling up overview previous features a crawler must contents index 2008 cambridge university press this is an automatically generated page in case of formatting errors you may want to look at the pdf edition of the book 2009 04 07
