features a crawler must provide next up previous contents index next features a crawler should up overview previous overview contents index features a crawler must provide we list the desiderata for web crawlers in two categories features that web crawlers must provide followed by features they should provide robustness the web contains servers that create spider traps which are generators of web pages that mislead crawlers into getting stuck fetching an infinite number of pages in a particular domain crawlers must be designed to be resilient to such traps not all such traps are malicious some are the inadvertent side effect of faulty website development politeness web servers have both implicit and explicit policies regulating the rate at which a crawler can visit them these politeness policies must be respected 2008 cambridge university press this is an automatically generated page in case of formatting errors you may want to look at the pdf edition of the book 2009 04 07
