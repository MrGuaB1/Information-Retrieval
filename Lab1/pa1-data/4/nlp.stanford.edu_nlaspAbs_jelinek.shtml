the stanford nlp natural language processing group the stanford natural language processing group home people teaching research publications software events local language modeling by random forests fred jelinek johns hopkins university abstract automatic speech recognition is based on several components signal processor acoustic model language model and search in this talk we explore the use of random forests rfs in language modeling the problem of predicting the next word based on words already seen the goal is to develop a new language model smoothing technique based on randomly grown decision trees dts this new technique is complementary to many of the existing techniques dealing with data sparseness random forests were studied by breiman in the context of classification into a relatively small number of classes we study their application to n gram language modeling which could be thought of as classification into a very large number of classes unlike regular n gram language models rf language models have the potential to generalize well to unseen data even when histories are long 4 we show that our rf language models are superior to regular n gram language models in reducing both the perplexity ppl and word error rate wer in a large vocabulary speech recognizer the new technique developed in this work is general we will show that it works well when combined with other techniques including word clustering and the structured language model slm local links nlp lunch pail lunch nlp reading group javanlp javadocs machines wiki calendar site design by bill maccartney
