evaluation in information retrieval next up previous contents index next information retrieval system evaluation up irbook previous references and further reading contents index evaluation in information retrieval we have seen in the preceding chapters many alternatives in designing an ir system how do we know which of these techniques are effective in which applications should we use stop lists should we stem should we use inverse document frequency weighting information retrieval has developed as a highly empirical discipline requiring careful and thorough evaluation to demonstrate the superior performance of novel techniques on representative document collections in this chapter we begin with a discussion of measuring the effectiveness of ir systems section 8.1 and the test collections that are most often used for this purpose section 8.2 we then present the straightforward notion of relevant and nonrelevant documents and the formal evaluation methodology that has been developed for evaluating unranked retrieval results section 8.3 this includes explaining the kinds of evaluation measures that are standardly used for document retrieval and related tasks like text classification and why they are appropriate we then extend these notions and develop further measures for evaluating ranked retrieval results section 8.4 and discuss developing reliable and informative test collections section 8.5 we then step back to introduce the notion of user utility and how it is approximated by the use of document relevance section 8.6 the key utility measure is user happiness speed of response and the size of the index are factors in user happiness it seems reasonable to assume that relevance of results is the most important factor blindingly fast useless answers do not make a user happy however user perceptions do not always coincide with system designers notions of quality for example user happiness commonly depends very strongly on user interface design issues including the layout clarity and responsiveness of the user interface which are independent of the quality of the results returned we touch on other measures of the quality of a system in particular the generation of high quality result summary snippets which strongly influence user utility but are not measured in the basic relevance ranking paradigm section 8.7 subsections information retrieval system evaluation standard test collections evaluation of unranked retrieval sets evaluation of ranked retrieval results assessing relevance critiques and justifications of the concept of relevance a broader perspective system quality and user utility system issues user utility refining a deployed system results snippets references and further reading next up previous contents index next information retrieval system evaluation up irbook previous references and further reading contents index 2008 cambridge university press this is an automatically generated page in case of formatting errors you may want to look at the pdf edition of the book 2009 04 07
