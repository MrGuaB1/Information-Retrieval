overview next up previous contents index next features a crawler must up web crawling and indexes previous web crawling and indexes contents index overview web crawling is the process by which we gather pages from the web in order to index them and support a search engine the objective of crawling is to quickly and efficiently gather as many useful web pages as possible together with the link structure that interconnects them in chapter 19 we studied the complexities of the web stemming from its creation by millions of uncoordinated individuals in this chapter we study the resulting difficulties for crawling the web the focus of this chapter is the component shown in figure 19.7 as web crawler it is sometimes referred to as a spider the goal of this chapter is not to describe how to build the crawler for a full scale commercial web search engine we focus instead on a range of issues that are generic to crawling from the student project scale to substantial research projects we begin section 20.1 1 by listing desiderata for web crawlers and then discuss in section 20.2 how each of these issues is addressed the remainder of this chapter describes the architecture and some implementation details for a distributed web crawler that satisfies these features section 20.3 discusses distributing indexes across many machines for a web scale implementation subsections features a crawler must provide features a crawler should provide next up previous contents index next features a crawler must up web crawling and indexes previous web crawling and indexes contents index 2008 cambridge university press this is an automatically generated page in case of formatting errors you may want to look at the pdf edition of the book 2009 04 07
