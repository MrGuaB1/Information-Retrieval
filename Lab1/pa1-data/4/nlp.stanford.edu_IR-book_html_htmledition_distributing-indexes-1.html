distributing indexes next up previous contents index next connectivity servers up web crawling and indexes previous the url frontier contents index distributing indexes in section 4.4 we described distributed indexing we now consider the distribution of the index across a large computer cluster that supports querying two obvious alternative index implementations suggest themselves partitioning by terms also known as global index organization and partitioning by documents also know as local index organization in the former the dictionary of index terms is partitioned into subsets each subset residing at a node along with the terms at a node we keep the postings for those terms a query is routed to the nodes corresponding to its query terms in principle this allows greater concurrency since a stream of queries with different query terms would hit different sets of machines in practice partitioning indexes by vocabulary terms turns out to be non trivial multi word queries require the sending of long postings lists between sets of nodes for merging and the cost of this can outweigh the greater concurrency load balancing the partition is governed not by an a priori analysis of relative term frequencies but rather by the distribution of query terms and their co occurrences which can drift with time or exhibit sudden bursts achieving good partitions is a function of the co occurrences of query terms and entails the clustering of terms to optimize objectives that are not easy to quantify finally this strategy makes implementation of dynamic indexing more difficult a more common implementation is to partition by documents each node contains the index for a subset of all documents each query is distributed to all nodes with the results from various nodes being merged before presentation to the user this strategy trades more local disk seeks for less inter node communication one difficulty in this approach is that global statistics used in scoring such as idf must be computed across the entire document collection even though the index at any single node only contains a subset of the documents these are computed by distributed background processes that periodically refresh the node indexes with fresh global statistics how do we decide the partition of documents to nodes based on our development of the crawler architecture in section 20.2 1 one simple approach would be to assign all pages from a host to a single node this partitioning could follow the partitioning of hosts to crawler nodes a danger of such partitioning is that on many queries a preponderance of the results would come from documents at a small number of hosts and hence a small number of index nodes a hash of each url into the space of index nodes results in a more uniform distribution of query time computation across nodes at query time the query is broadcast to each of the nodes with the top results from each node being merged to find the top documents for the query a common implementation heuristic is to partition the document collection into indexes of documents that are more likely to score highly on most queries using for instance techniques in chapter 21 and low scoring indexes with the remaining documents we only search the low scoring indexes when there are too few matches in the high scoring indexes as described in section 7.2 1 next up previous contents index next connectivity servers up web crawling and indexes previous the url frontier contents index 2008 cambridge university press this is an automatically generated page in case of formatting errors you may want to look at the pdf edition of the book 2009 04 07
