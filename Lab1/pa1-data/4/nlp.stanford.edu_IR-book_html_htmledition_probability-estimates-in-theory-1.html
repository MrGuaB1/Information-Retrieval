probability estimates in theory next up previous contents index next probability estimates in practice up the binary independence model previous deriving a ranking function contents index probability estimates in theory for each term what would these numbers look like for the whole collection odds ratio ct contingency gives a contingency table of counts of documents in the collection where is the number of documents that contain term using this and and 74 to avoid the possibility of zeroes such as if every or no relevant document has a particular term it is fairly standard to add to each of the quantities in the center 4 terms of odds ratio ct contingency and then to adjust the marginal counts the totals accordingly so the bottom right cell totals then we have 75 adding in this way is a simple form of smoothing for trials with categorical outcomes such as noting the presence or absence of a term one way to estimate the probability of an event from data is simply to count the number of times an event occurred divided by the total number of trials this is referred to as the relative frequency of the event estimating the probability as the relative frequency is the maximum likelihood estimate or mle because this value makes the observed data maximally likely however if we simply use the mle then the probability given to events we happened to see is usually too high whereas other events may be completely unseen and giving them as a probability estimate their relative frequency of 0 is both an underestimate and normally breaks our models since anything multiplied by 0 is 0 simultaneously decreasing the estimated probability of seen events and increasing the probability of unseen events is referred to as smoothing one simple way of smoothing is to add a number to each of the observed counts these pseudocounts correspond to the use of a uniform distribution over the vocabulary as a bayesian prior following equation 59 we initially assume a uniform distribution over events where the size of denotes the strength of our belief in uniformity and we then update the probability based on observed events since our belief in uniformity is weak we use this is a form of maximum a posteriori map estimation where we choose the most likely point value for probabilities based on the prior and the observed evidence following equation 59 we will further discuss methods of smoothing estimated counts to give probability models in section 12.2 2 page the simple method of adding to each observed count will do for now next up previous contents index next probability estimates in practice up the binary independence model previous deriving a ranking function contents index 2008 cambridge university press this is an automatically generated page in case of formatting errors you may want to look at the pdf edition of the book 2009 04 07
