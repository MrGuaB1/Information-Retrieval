time complexity of hac next up previous contents index next group average agglomerative clustering up single link and complete link clustering previous single link and complete link clustering contents index time complexity of hac the complexity of the naive hac algorithm in figure 17.2 is because we exhaustively scan the matrix for the largest similarity in each of iterations for the four hac methods discussed in this chapter a more efficient algorithm is the priority queue algorithm shown in figure 17.8 its time complexity is the rows of the similarity matrix are sorted in decreasing order of similarity in the priority queues then returns the cluster in that currently has the highest similarity with where we use to denote the cluster as in chapter 16 after creating the merged cluster of and is used as its representative the function sim computes the similarity function for potential merge pairs largest similarity for single link smallest similarity for complete link average similarity for gaac section 17.3 and centroid similarity for centroid clustering section 17.4 we give an example of how a row of is processed figure 17.8 bottom panel the loop in lines 1 7 is and the loop in lines 9 21 is for an implementation of priority queues that supports deletion and insertion in the overall complexity of the algorithm is therefore in the definition of the function sim and are the vector sums of and respectively and and are the number of documents in and respectively the argument of efficienthac in figure 17.8 is a set of vectors as opposed to a set of generic documents because gaac and centroid clustering and 17.4 require vectors as input the complete link version of efficienthac can also be applied to documents that are not represented as vectors for single link we can introduce a next best merge array nbm as a further optimization as shown in figure 17.9 nbm keeps track of what the best merge is for each cluster each of the two top level for loops in figure 17.9 are thus the overall complexity of single link clustering is can we also speed up the other three hac algorithms with an nbm array we cannot because only single link clustering is best merge persistent suppose that the best merge cluster for is in single link clustering then after merging with a third cluster the merge of and will be s best merge cluster exercise 17.10 in other words the best merge candidate for the merged cluster is one of the two best merge candidates of its components in single link clustering this means that can be updated in in each iteration by taking a simple max of two values on line 14 in figure 17.9 for each of the remaining clusters figure 17.10 demonstrates that best merge persistence does not hold for complete link clustering which means that we cannot use an nbm array to speed up clustering after merging s best merge candidate with cluster an unrelated cluster becomes the best merge candidate for this is because the complete link merge criterion is non local and can be affected by points at a great distance from the area where two merge candidates meet in practice the efficiency penalty of the algorithm is small compared with the single link algorithm since computing the similarity between two documents eg as a dot product is an order of magnitude slower than comparing two scalars in sorting all four hac algorithms in this chapter are with respect to similarity computations so the difference in complexity is rarely a concern in practice when choosing one of the algorithms exercises show that complete link clustering creates the two cluster clustering depicted in figure 17.7 next up previous contents index next group average agglomerative clustering up single link and complete link clustering previous single link and complete link clustering contents index 2008 cambridge university press this is an automatically generated page in case of formatting errors you may want to look at the pdf edition of the book 2009 04 07
