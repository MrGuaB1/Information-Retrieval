folding home faq flops home download guides faq stats science results awards about us main news forum help pande group donate albanian   dansk deutsch english espanol francais italiano      lietuviu magyar nederlands norsk occitan  polski portugues  suomeksi svenska turkish tieng viet site search folding home flop faq table of contents what is a flop what is a tflop or teraflop are flops consistent between different types of architectures what are x86 flops what are native flops is there a way to account for these differences to allow for an apples to apples comparison why are the ati x86 flop numbers half of the ati native flop numbers why are the x86 flop counts generally higher what conversion did you use for gpu to x86 flops why aren t the differences between native gpu and x86 flop counts even greater what is a flop flop is an acronym for floating point operation often one refers to the flops meaning the floating point operations per second the flops is a measure of a computer's performance especially in fields of scientific calculations that make heavy use of floating point calculations what is a tflop or teraflop a teraflop is a trillion flops or 10 12 flops most supercomputers are of a teraflop scale whereas the next generation are designed to be in the petaflop 10 15 flops scale please note fah already runs in the petaflop scale are flops consistent between different types of architectures no different hardware can compute the same quantities differently typically an addition or a multiply operation is always considered to be one flop but more complex functions such as exponential sine or cosine are less clear what are x86 flops x86 flops are an estimate of how many flops a given calculation would take on an x86 class cpu what are native flops we refer to the flop count on a given hardware as the native flop count for example an exponential on a gpu is one native gpu flop but many native x86 flops is there a way to account for these differences to allow for an apples to apples comparison one of the most common ways to do this is to use the same flop count for a given instruction no matter how the native hardware performs due to its ubiquity the x86 architecture is often used as the reference architecture for this purpose this makes sense in folding home as well due to the ubiquity of x86 hardware involved in the project why are the ati x86 flop numbers half of the ati native flop numbers due to a difference in the implementation in part due to hardware differences the ati code must do two force calculations where the x86 cell and nvidia hardware need only do one this increases the overall native flop count for ati hardware but since these are not useful flops in a sense we did not include them in the x86 count why are the x86 flop counts generally higher the gpus are more efficient at certain mathematical functions eg exponential sine and cosine and thus native gpu flop counts underestimate their performance relative to x86 hardware in our x86 flop count metric we have attempted to even this out what conversion did you use for gpu to x86 flops for native gpu flops we tried to count each operation as just one flop so a sqrt is one flop and a reciprocal sqrt is two flops etc which leads to native gpu flop counts of the form sqrt 1 rsqrt 2 log 1 exp 1 acos 1 cos 1 sin 1 dot3 5 cross3 9 for x86 flops we followed http ai stanford edu paskin slam javadoc javaslam util flops html which for example leads to x86 flop counts sqrt 15 rsqrt 16 log 20 exp 20 why aren t the differences between native gpu and x86 flop counts even greater in the end much of the code uses simpler operations add multiply divide etc which counts as one flop under both systems the instructions which are much more different eg exp x are more rare say 1 10th the number of instructions and thus the overall difference is closer to 2x for more information fah faq folding support forum last updated on april 04 2009 at 11 56 pm
