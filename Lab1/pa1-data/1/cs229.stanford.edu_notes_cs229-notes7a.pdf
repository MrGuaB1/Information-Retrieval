cs229 lecture notes page 1 cs229 lecture notes andrew ng the k means clustering algorithm in the clustering problem we are given a training set x 1 x m and want to group the data into a few cohesive clusters here x i rn as usual but no labels y i are given so this is an unsupervised learning problem the k means clustering algorithm is as follows 1 initialize cluster centroids 1 2 k rn randomly 2 repeat until convergence for every i set c i arg min j x i j 2 for each j set j m i 1 1 c i j x i m i 1 1 c i j in the algorithm above k a parameter of the algorithm is the number of clusters we want to find and the cluster centroids j represent our current guesses for the positions of the centers of the clusters to initialize the cluster centroids in step 1 of the algorithm above we could choose k training examples randomly and set the cluster centroids to be equal to the values of these k examples other initialization methods are also possible the inner loop of the algorithm repeatedly carries out two steps i assigning each training example x i to the closest cluster centroid j and ii moving each cluster centroid j to the mean of the points assigned to it figure 1 shows an illustration of running k means 1 page 2 2 a b c d e f figure 1 k means algorithm training examples are shown as dots and cluster centroids are shown as crosses a original dataset b random ini tial cluster centroids in this instance not chosen to be equal to two training examples cf illustration of running two iterations of k means in each iteration we assign each training example to the closest cluster centroid shown by painting the training examples the same color as the cluster centroid to which is assigned then we move each cluster centroid to the mean of the points assigned to it best viewed in color images courtesy michael jordan is the k means algorithm guaranteed to converge yes it is in a certain sense in particular let us define the distortion function to be j c  m i 1 x i c i 2 thus j measures the sum of squared distances between each training exam ple x i and the cluster centroid c i to which it has been assigned it can be shown that k means is exactly coordinate descent on j specifically the inner loop of k means repeatedly minimizes j with respect to c while holding  fixed and then minimizes j with respect to  while holding c fixed thus j must monotonically decrease and the value of j must converge usu ally this implies that c and  will converge too in theory it is possible for page 3 3 k means to oscillate between a few different clusterings ie a few different values for c and or  that have exactly the same value of j but this almost never happens in practice the distortion function j is a non convex function and so coordinate descent on j is not guaranteed to converge to the global minimum in other words k means can be susceptible to local optima very often k means will work fine and come up with very good clusterings despite this but if you are worried about getting stuck in bad local minima one common thing to do is run k means many times using different random initial values for the cluster centroids j then out of all the different clusterings found pick the one that gives the lowest distortion j c 
