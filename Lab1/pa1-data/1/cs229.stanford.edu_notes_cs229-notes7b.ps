cs229 lecture notes page 1 cs229 lecture notes andrew ng mixtures of gaussians and the em algorithm in this set of notes we discuss the em expectation maximization for den sity estimation suppose that we are given a training set x 1 x m as usual since we are in the unsupervised learning setting these points do not come with any labels we wish to model the data by specifying a joint distribution p x i z i p x i z i p z i here z i multinomial  where j 0 k j 1 j 1 and the parameter j gives p z i j and x i z i j n j j we let k denote the number of values that the z i s can take on thus our model posits that each x i was generated by randomly choosing z i from 1 k and then x i was drawn from one of k gaussians depeneding on z i this is called the mixture of gaussians model also note that the z i s are latent random variables meaning that they re hidden unobserved this is what will make our estimation problem difficult the parameters of our model are thus   and  to estimate them we can write down the likelihood of our data l    m i 1 log p x i    m i 1 log k z i 1 p x i z i   p z i  however if we set to zero the derivatives of this formula with respect to the parameters and try to solve we ll find that it is not possible to find the maximum likelihood estimates of the parameters in closed form try this yourself at home the random variables z i indicate which of the k gaussians each x i had come from note that if we knew what the z i s were the maximum 1 page 2 2 likelihood problem would have been easy specifically we could then write down the likelihood as l    m i 1 log p x i z i   log p z i  maximizing this with respect to   and  gives the parameters j 1 m m i 1 1 z i j j m i 1 1 z i j x i m i 1 1 z i j j m i 1 1 z i j x i j x i j t m i 1 1 z i j indeed we see that if the z i s were known then maximum likelihood estimation becomes nearly identical to what we had when estimating the parameters of the gaussian discriminant analysis model except that here the z i s playing the role of the class labels 1 however in our density estimation problem the z i s are not known what can we do the em algorithm is an iterative algorithm that has two main steps applied to our problem in the e step it tries to guess the values of the z i s in the m step it updates the parameters of our model based on our guesses since in the m step we are pretending that the guesses in the first part were correct the maximization becomes easy here's the algorithm repeat until convergence e step for each i j set w i j p z i j x i    1there are other minor differences in the formulas here from what we d obtained in ps1 with gaussian discriminant analysis first because we ve generalized the z i s to be multinomial rather than bernoulli and second because here we are using a different j for each gaussian page 3 3 m step update the parameters j 1 m m i 1 w i j j m i 1 w i j x i m i 1 w i j j m i 1 w i j x i j x i j t m i 1 w i j in the e step we calculate the posterior probability of our parameters the z i s given the x i and using the current setting of our parameters ie using bayes rule we obtain p z i j x i    p x i z i j   p z i j  k l 1 p x i z i l   p z i l  here p x i z i j   is given by evaluating the density of a gaussian with mean j and covariance j at x i p z i j  is given by j and so on the values w i j calculated in the e step represent our soft guesses2 for the values of z i also you should contrast the updates in the m step with the formulas we had when the z i s were known exactly they are identical except that in stead of the indicator functions 1 z i j indicating from which gaussian each datapoint had come we now instead have the w i j s the em algorithm is also reminiscent of the k means clustering algo rithm except that instead of the hard cluster assignments c i we instead have the soft assignments w i j similar to k means it is also susceptible to local optima so reinitializing at several different initial parameters may be a good idea it's clear that the em algorithm has a very natural interpretation of repeatedly trying to guess the unknown z i s but how did it come about and can we make any guarantees about it such as regarding its convergence in the next set of notes we will describe a more general view of em one 2the term soft refers to our guesses being probabilities and taking values in 0 1 in contrast a hard guess is one that represents a single best guess such as taking values in 0 1 or 1 k page 4 4 that will allow us to easily apply it to other estimation problems in which there are also latent variables and which will allow us to give a convergence guarantee
