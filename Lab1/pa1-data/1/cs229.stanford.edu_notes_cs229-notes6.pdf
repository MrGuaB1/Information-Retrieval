cs229 lecture notes page 1 cs229 lecture notes andrew ng 1 the perceptron and large margin classifiers in this final set of notes on learning theory we will introduce a different model of machine learning specifically we have so far been considering batch learning settings in which we are first given a training set to learn with and our hypothesis h is then evaluated on separate test data in this set of notes we will consider the online learning setting in which the algorithm has to make predictions continuously even while it's learning in this setting the learning algorithm is given a sequence of examples x 1 y 1 x 2 y 2 x m y m in order specifically the algorithm first sees x 1 and is asked to predict what it thinks y 1 is after making its pre diction the true value of y 1 is revealed to the algorithm and the algorithm may use this information to perform some learning the algorithm is then shown x 2 and again asked to make a prediction after which y 2 is revealed and it may again perform some more learning this proceeds until we reach x m y m in the online learning setting we are interested in the total number of errors made by the algorithm during this process thus it models applications in which the algorithm has to make predictions even while it's still learning we will give a bound on the online learning error of the perceptron algo rithm to make our subsequent derivations easier we will use the notational convention of denoting the class labels by y 1 1 recall that the perceptron algorithm has parameters  rn 1 and makes its predictions according to h x g t x 1 where g z 1 if z 0 1 if z 0 1 page 2 cs229 winter 2003 2 also given a training example x y the perceptron learning rule updates the parameters as follows if h x y then it makes no change to the parameters otherwise it performs the update1   yx the following theorem gives a bound on the online learning error of the perceptron algorithm when it is run as an online algorithm that performs an update each time it gets an example wrong note that the bound below on the number of errors does not have an explicit dependence on the number of examples m in the sequence or on the dimension n of the inputs theorem block 1962 and novikoff 1962 let a sequence of exam ples x 1 y 1 x 2 y 2 x m y m be given suppose that x i d for all i and further that there exists a unit length vector u u 2 1 such that y i ut x i  for all examples in the sequence ie ut x i  if y i 1 and ut x i  if y i 1 so that u separates the data with a margin of at least  then the total number of mistakes that the perceptron algorithm makes on this sequence is at most d  2 proof the perceptron updates its weights only on those examples on which it makes a mistake let  k be the weights that were being used when it made its k th mistake so  1 0 since the weights are initialized to zero and if the k th mistake was on the example x i y i then g x i t  k y i which implies that x i t  k y i 0 2 also from the perceptron learning rule we would have that  k 1  k y i x i we then have  k 1 t u  k t u y i x i t u  k t u  by a straightforward inductive argument implies that  k 1 t u k 3 1this looks slightly different from the update rule we had written down earlier in the quarter because here we have changed the labels to be y 1 1 also the learning rate parameter  was dropped the only effect of the learning rate is to scale all the parameters  by some fixed constant which does not affect the behavior of the perceptron page 3 cs229 winter 2003 3 also we have that  k 1 2  k y i x i 2  k 2 x i 2 2y i x i t  i  k 2 x i 2  k 2 d2 4 the third step above used equation 2 moreover again by applying a straightfoward inductive argument we see that 4 implies  k 1 2 kd2 5 putting together 3 and 4 we find that kd  k 1  k 1 t u k the second inequality above follows from the fact that u is a unit length vector and zt u z u cos  z u where  is the angle between z and u our result implies that k d  2 hence if the perceptron made a k th mistake then k d  2 d
