icme gpu cluster tutorial getting started on the icme gpu cluster this document is intended to help you get started with running cuda mex matlab executable jobs on the icme gpu cluster this tutorial is organized as follows setting up your bash environment compiling cuda files compiling mex with cuda using torque to schedule jobs references notes this tutorial is targetted towards c c++ but the initial steps should be common to all there are a number of text editors available on the cluster such as vim gedit etc the default matlab path is assumed to be cm shared apps matlab if this changes you can find the default matlab path by typing matlabpool in matlab setting up your bash environment to enable cuda torque mpi matlab by default add the following lines to your bashrc file module add open64 module add openmpi module add cuda40 module add torque export path cm shared apps matlab bin path now you can call the nvidia cuda compiler directly by typing nvcc and run matlab by typing matlab to enable support for running mex files add the following to your bashrc file export ld_library_path cm shared apps matlab extern lib glnxa64 ld_library_path export includepath cm shared apps matlab extern include includepath this will ensure all the libraries are available in your path when running mex files compiling cuda files each node on the icme gpu cluster has multiple gpus 6 to 7 so you need to specify which gpu to select via your code the following lines should be included in the start of your program in order to do this cudagetlasterror cudasetdevice rank where rank usually ranges from 0 to 5 compiling mex with cuda the following lines show how to compile a file mexcuda cpp which is a matlab executable file that contains cuda kernels dir cd dirname $0 && pwd matlabroot cm shared apps matlab nvcc c dir mexcuda cu o dir mexcuda o xcompiler fpic i matlabroot extern include i cm shared apps cuda40 toolkit 4.0 17 include arch sm_13 o3 m64 mex dir mexcuda o i cm shared apps cuda40 toolkit 4.0 17 include l cm shared apps cuda40 toolkit 4.0 17 lib64 lm lcudart cxx this will result in mexcuda mexa64 file that can be called directly from matlab using moab to schedule jobs scheduling jobs on torque is relatively straightforward and there are many excellent resources available online for this 3 5 6 here is a brief overview of some of the commonly used functions msub used to add jobs to the queue which can either be in the form of a bash script or an interactive job showq displays the status of the queue such as scheduled running jobs and their details mdel used to delete jobs from the queue to run an interactive job use msub i to run a job as a bash script my script sh on 1 node 2 processors per node and 4 gpus use msub my script sh lnodes 1 ppn 2 gpus 4 to delete a job type mdel followed by the job id obtained via qstat you only have permission to delete your own jobs more examples example from diwakar shukla job submission script bin bash msub n test msub e home shukla test run err msub o home shukla test run out msub l nodes 1 ppn 5 gpus 5 msub l walltime 72 00 00 msub v msub q longq execute commands cd home shukla test python test1 py test1 out & python test2 py test2 out & python test3 py test3 out & python test4 py test4 out & python test5 py test5 out wait test py openmm python script for running md simulations from simtk openmm app import from simtk openmm import from simtk unit import temperature 350 kelvin cuda openmm platform_getplatform 1 pdb pdbfile test_initial pdb deviceid 4 cuda setpropertydefaultvalue cudadevice d deviceid select cuda device index print deviceid s deviceid forcefield forcefield amber96 xml amber96_obc xml system forcefield createsystem pdb topology nonbondedcutoff 1 nanometer constraints hangles integrator langevinintegrator temperature 91 picosecond 0.002 picoseconds simulation simulation pdb topology system integrator simulation context setpositions pdb positions print platform s simulation context getplatform getname simulation minimizeenergy tolerance 1.0 kilocalories_per_mole simulation reporters append pdbreporter test pdb 500000 simulation step 50000000 references here are some links to additional resources 1 torque administrator guide 2 bright cluster manager user manual 3 moab workload manager administrator guide 4 torque resource manager qsub 5 how to use sun grid engine 6 tutorial submitting a job using qsub contact please contact brian tempero for issues related to the cluster tutorial written by aditya khosla
