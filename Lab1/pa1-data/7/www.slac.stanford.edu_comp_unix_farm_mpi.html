mpi tutorial computing at slac search slac slac home computing home unix at slac high performance computing at slac parallel computing at slac mpi tutorial mpi's hello world name this little hello world program hello c include stdio h include mpi h int main int argc char argv int numprocs rank namelen char processor_name mpi_max_processor_name mpi_init &argc &argv mpi_comm_size mpi_comm_world &numprocs mpi_comm_rank mpi_comm_world &rank mpi_get_processor_name processor_name &namelen printf process d on s out of d n rank processor_name numprocs mpi_finalize compiling and linking mpi programs once you have your mpi example program you can compile and link it with the mpi version that supports the network type you need linux with ethernet or infiniband openmpi mpicc hello c o hello open mpi is using the following environment variables for alternative compilers c ompi_cc c++ ompi_cxx fortran 77 ompi_f77 fortran 90 ompi_fc the comple list of environment variables for compilers compiler flags linkters and linkger flags can be found in the openmpi faq running mpi programs logon to the pinto cluster ssh pinto make sure your mpi program is compiled for these machines rhel5 add the machines you want your parallel program to run on to a file alfw pinto cat machinefile pinto pinto01 pinto01 pinto02 pinto02 this will first use pinto01 and then use pinto02 if you need more processes than cpus are in your machinefile mpi will start again at the top of the file to run your program interactively execute one of these commands openmpi on linux make sure the path to all openmpi commands gets added automatically at login time you may use mpi selector to establish your preference linux with ethernet alfw pinto mpirun np 4 machinefile machinefile pinto hello process 0 on pinto01 out of 4 process 1 on pinto01 out of 4 process 3 on pinto02 out of 4 process 2 on pinto02 out of 4 linux without infiniband or lsf if you want to run your openmpi code on lets say a cluster of desktop machines where you don t have infiniband or lsf you should use these parameters on your mpirun command mpirun mca btl ib mca pls_rsh_agent ssh np 4 machinefile machinefile desktops hello this will exclude the infiniband driver ib as a choice for the communication network and will use ssh instead of the lsf wrapper to start your programs on the remote machines lsf integration and usage currently there is one lsf queue defined for running mpi jobs via lsf the mpi ibq the mpi ibq has been designated as the default production mpi queue and is configured to use the 32 node pinto infiniband cluster presently the mpi ibq is setup to use a maximum of 8 processors per machine which gives this queue a total of 256 available processors before you submit an mpi job to lsf make sure you have the correct mpirun binary in your path environment variable on the pinto cluster this is accomplished by using mpi selector the system default is set for the cluster but you may choose to use a different version of openmpi and prefer to set a user level default to submit your mpi programs via lsf to either cluster you have to compile your mpi jobs just as you would for interactive job launch instead of using mpirun for program start you have to use the following lsf command from either a morab or norica pinto machine to submit jobs to the default mpi ibq pinto cluster bsub a mympi n number of processors openmpi_program where a mympi is the esub required to run openmpi jobs example bsub a mympi n 10 prog openmpi bin hello++ each mpi queue by default is configured with the span resource empty span to allow jobs to be scheduled on machines that have an empty processor based on normal lsf load considerations if you wish to specify a specific spanning resource option to suit your parallel batch job you may use r span ptile value on the bsub command where value is the number of processors you wish to use on each host example bsub a mympi n 10 r span ptile 2 prog openmpi bin hello++ in this example span ptile 2 will require the job to run on two processors per host some jobs may perform better when run on a single processor per host in this case you would want to specify r span ptile 1 consider that we currently have 64 hosts allocated to the mpiq if you use span ptile 1 the n number of processors option on bsub could be no greater than 32 for your job to run example bsub a mympi n 32 r span ptile 1 prog openmpi bin hello++ submitting an openmpi job to use the old package verson of openmpi job presently works by using the a openmpi parameter example bsub q mpi_ibq a openmpi n 10 prog openmpi bin hello++ references mpi standard open mpi home page neal adams and alf wachsmann last modified oct 22 2010
