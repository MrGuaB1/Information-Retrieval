two minute overview of slac's unix batch system 4feb2004 here are a few pointers on getting started with unix batch computing using the scs unix compute farm the compute farm is under the responsibility of scs high performance computing team aka the hpc team unix batch is controlled by a system called lsf load sharing facility for handling job scheduling queues etc all of the batch machines offered by scs under lsf start with the name bronco for sun solaris systems and either barb noma or tori for redhat linux systems though other groups may have machines available for their own use under lsf note lsf is only available on systems that are so licensed this includes any of the interactive compute farm machines with the generic name of tersk for sun solaris systems and noric for redhat linux systems and the x hosts currently a set of machines with the generic name of flora for sun solaris systems use a web browser to look at http www slac stanford edu comp unix unix hpc html for more information but in the meantime since you re probably in a hurry here are some ways to learn more if you read anything at all please read this part to try running a batch job simply give the command bsub c0 1 hostname this will submit a trivial batch job c0 1 means i ve requested a maximum of 0 hours and 1 minute of slac cpu time that consists of just the unix command hostname which just prints out the name of the host computer on which the job runs try it out you ll get a message like job 277785 is submitted to default queue short then in a short amount of time unless the batch system is terribly overloaded you ll get output back in your mail that will look something like this date thu 06 jul 2000 17 16 09 0700 pdt from lsf lsf slac stanford edu subject job 277785 hostname done sender lsf system lsf slac stanford edu to randym slac stanford edu message id 200007070016 raa13255 bronco426 slac stanford edu job hostname was submitted from host tersk01 by user randym job was executed on host s bronco426 in queue short as user randym u sf randym was used as the home directory u sf randym was used as the working directory started at thu jul 6 17 16 00 2000 results reported at thu jul 6 17 16 09 2000 your job looked like lsbatch user input hostname successfully completed resource usage summary cpu time 0.77 sec max memory 2 mb max swap 4 mb max processes 1 the output if any follows bronco426 you can use the bjobs command to ask the system about the status of any submitted jobs that are not yet finished easy right read on for those still wanting more information 1 you can use the man command to learn about lsf by typing man lsfintro this will in turn give pointers to other commands such as bsub that will be useful also man lsfbatch has useful information 2 you can also try man xlsbatch to learn about a graphical user interface to much of the job submission system the xlsbatch command runs in any x windowing environment that is also licensed for lsf 3 more information can be found on the web at http www slac stanford edu comp unix farm farm html including a version of slac's introduction to the unix cpu compute farm 4 the hpc team member responsible for lsf is neal adams ext 2821 neal slac stanford edu neal is always interested in helping people use lsf or listening to suggestions about configuration changes wishlist features etc 5 the hpc team also has a news group called slac comp computefarm items are posted there that are not time critical in nature and or fit into an environment suitable for dialog i hope that should be enough to get started after you ve read a little bit you might want to simply try the xlsbatch command with a graphical user interface and submit a trivial job or two even just a command if you wish to get some experience and of course i m always interested in feedback there is also a mechanism for dealing with large staged files on working disk space that reside on silo cartridges but that's another story if you d prefer to talk to a colleague who has started this here are some people who have agreed to help charlie young young slac stanford edu tom glanzman dragon slac stanford edu charlotte hee chee slac stanford edu randy melen scs high performance computing team randym slac stanford edu x2841
