ee364b homework 2 page 1 ee364b prof s boyd ee364b homework 2 1 subgradient optimality conditions for nondifferentiable inequality constrained optimiza tion consider the problem minimize f0 x subject to fi x 0 i 1 m with variable x rn we do not assume that f0 fm are convex suppose that x and  0 satisfy primal feasibility fi x 0 i 1 m dual feasibility 0 f0 x m i 1 i fi x and the complementarity condition ifi x 0 i 1 m show that x is optimal using only a simple argument and definition of subgradient recall that we do not assume the functions f0 fm are convex 2 optimality conditions and coordinate wise descent for l1 regularized minimization we consider the problem of minimizing  x f x x1 where f rn r is convex and differentiable and  0 the number  is the regularization parameter and is used to control the trade off between small f and small x1 when l1 regularization is used as a heuristic for finding a sparse x for which f x is small  controls roughly the trade off between f x and the cardinality number of nonzero elements of x a show that x 0 is optimal for this problem ie minimizes  if and only if f 0  in particular for  max f 0 l1 regularization yields the sparsest possible x the zero vector remark the value max gives a good reference point for choosing a value of the penalty parameter  in l1 regularized minimization a common choice is to start with  max 2 and then adjust  to achieve the desired sparsity fit trade off 1 page 2 b coordinate wise descent in the coordinate wise descent method for minimizing a convex function g we first minimize over x1 keeping all other variables fixed then we minimize over x2 keeping all other variables fixed and so on after minimizing over xn we go back to x1 and repeat the whole process repeatedly cycling over all n variables show that coordinate wise descent fails for the function g x x1 x2 0.1 x1 x2 in particular verify that the algorithm terminates after one step at the point x 0 2 x 0 2 while infx g x thus coordinate wise descent need not work for general convex functions c now consider coordinate wise descent for minimizing the specific function  de fined above assuming f is strongly convex say it can be shown that the iterates converge to a fixed point x show that x is optimal ie minimizes  thus coordinate wise descent works for l1 regularized minimization of a differ entiable function d work out an explicit form for coordinate wise descent for l1 regularized least squares ie for minimizing the function ax b2 2 x1 you might find the deadzone function  u u 1 u 1 0 u 1 u 1 u 1 useful generate some data and try out the coordinate wise descent method check the result against the solution found using cvx and produce a graph show ing convergence of your coordinate wise method 2
