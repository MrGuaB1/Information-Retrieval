quiz 1 page 1 cs224n natural language processing spring 2008 weekly quiz quiz 1 1 conditional probability which of the following is always true there are two random variables x and y sumx p x y 1 sumy p x y 1 sumx y p x y 1 all of the above none of the above 2 perplexity suppose a language model assigns the following conditional n gram probabilities to a 3 word test set 1 4 1 2 1 4 then p test set 1 4 1 2 1 4 0.03125 what is the perplexity 2.5 1.5 2.828 0.75 3.175 3 which one of the following is true after smoothing a probability distribution might not sum up to 1 anymore entropy of a discrete random variable is always non negative the problem of add one smoothing is that unseen events don t get enough probability mass p a b p a p b none of the above 4 for the following questions assume we are using a corpus completely summarized by the unigram counts below thus v 20 unigram counts brown 29 fox 34 lazy 18 dog 1 page 2 plenty 41 tree 1 skim 4 neat 49 syzygy 33 missing 12 napkin 9 cheap 22 fork 10 nickel 1 chocolate 5 syrup 9 short 28 options 13 car 14 concinnity 0 sum 333 what are the following probabilities answer as a fraction or a whole number eg 1 2 or 1 pmle short 28 333 13 333 28 353 28 334 5 pmle concinnity 1 333 1 353 1 334 0 6 now assume we are using laplace smoothing what are the following probabilities plaplace lazy 18 353 19 353 19 334 18 334 7 plaplace concinnity page 3 0 1 353 1 333 1 334 8 now assume we are using good turing smoothing what probability mass do we assign to things with zero frequency in our training data 1 111 0 3 353 3 20 9 now assume that the above counts were just drawn from a larger corpus and on that full corpus we collected the following counts of counts n1 112849 n2 41018 n3 15608 n4 5704 n5 2111 n6 754 n7 283 n8 104 n9 37 n10 14 with this data what are the smoothed counts c for the following words assuming the unigram counts given above are still valid c skim 844 5704 22816 15608 10555 5704 10555 15608 10 c syrup 126 104 126 37 140 37 333 104 page 4
