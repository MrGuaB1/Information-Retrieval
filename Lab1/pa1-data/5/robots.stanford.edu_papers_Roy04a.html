sebastian thrun's homepage homepage research students courses robots papers videos press talks faq cv lab travel contact personal links finding approximate pomdp solutions through belief compression m roy g gordon and s thrun standard value function approaches to finding policies for partially observable markov decision processes pomdps are generally considered to be intractable for large models the intractability of these algorithms is to a large extent a consequence of computing an exact optimal policy over the entire belief space however in real world pomdp problems computing the optimal policy for the full belief space is often unnecessary for good control even for problems with complicated policy classes the beliefs experienced by the controller often lie near a structured low dimensional manifold embedded in the high dimensional belief space finding a good approximation to the optimal value function for only this manifold can be much easier than computing the full value function we introduce a new method for solving large scale pomdps by reducing the dimensionality of the belief space we use exponential family principal components analysis collins dasgupta & schapire 2002 to represent sparse high dimensional belief spaces using low dimensional sets of learned features of the belief state we then plan only in terms of the low dimensional belief features by planning in this low dimensional space we can find policies for pomdp models that are orders of magnitude larger than models that can be handled by conventional techniques we demonstrate the use of this algorithm on a synthetic problem and on mobile robot navigation tasks the full paper is available in pdf and gzipped postscript article roy04a author roy n and gordon g and thrun s title finding approximate pomdp solutions through belief compression journal journal of artificial intelligence research year 2004 note to appear
