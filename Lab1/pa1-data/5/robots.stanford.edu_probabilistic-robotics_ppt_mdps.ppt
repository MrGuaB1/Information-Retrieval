probabilistic robotics 1 sa 1 probabilistic robotics planning and control markov decision processes 2 problem classes deterministic vs stochastic actions full vs partial observability 3 deterministic fully observable 4 stochastic fully observable 5 stochastic partially observable 6 markov decision process mdp s 2 s 3 s 4 s 5 s 1 0.7 0.3 0.9 0.1 0.3 0.3 0.4 0.99 0.01 0.2 0.8 r 10 r 20 r 0 r 1 r 0 7 markov decision process mdp given states x actions u transition probabilities p x u x reward payoff function r x u wanted policy p x that maximizes the future expected reward 8 rewards and policies policy general case policy fully observable case expected cumulative payoff t 1 greedy policy t 1 finite horizon case typically no discount t infty infinite horizon case finite reward if discount 1 9 policies contd expected cumulative payoff of policy optimal policy 1 step optimal policy value function of 1 step optimal policy 10 2 step policies optimal policy value function 11 t step policies optimal policy value function 12 infinite horizon optimal policy bellman equation fix point is optimal policy necessary and sufficient condition 13 value iteration for all x do endfor repeat until convergence for all x do endfor endrepeat 14 value iteration for motion planning 15 value function and policy iteration often the optimal policy has been reached long before the value function has converged policy iteration calculates a new policy based on the current value function and then calculates a new value function based on this policy this process often converges faster to the optimal policy
