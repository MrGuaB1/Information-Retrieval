11.4 2 the negative log posterior page 1 354 11 the graphslam algorithm  p zt y0 t z1 t 1 u1 t c1 t p y0 t z1 t 1 u1 t c1 t where  is the familiar normalizer the first probability on the right hand side can be reduced by dropping irrelevant conditioning variables p zt y0 t z1 t 1 u1 t c1 t p zt yt ct 11.6 similarly we can factor the second probability by partitioning y0 t into xt and y0 t 1 and obtain p y0 t z1 t 1 u1 t c1 t 11.7 p xt y0 t 1 z1 t 1 u1 t c1 t p y0 t 1 z1 t 1 u1 t c1 t p xt xt 1 ut p y0 t 1 z1 t 1 u1 t 1 c1 t 1 putting these expressions back into 11.5 gives us the recursive definition of the full slam posterior p y0 t z1 t u1 t c1 t 11.8  p zt yt ct p xt xt 1 ut p y0 t 1 z1 t 1 u1 t 1 c1 t 1 the closed form expression is obtained through induction over t here p y0 is the prior over the map m and the initial pose x0 p y0 t z1 t u1 t c1 t  p y0 t p xt xt 1 ut p zt yt ct 11.9  p y0 t p xt xt 1 ut i p zi t yt ci t here as before zi t is the i th measurement in the measurement vector zt at time t the prior p y0 factors into two independent priors p x0 and p m in slam we usually have no prior knowledge about the map m we simply replace p y0 by p x0 and subsume the factor p m into the normalizer  11.4 2 the negative log posterior the information form represents probabilities in logarithmic form the log slam posterior follows directly from the previous equation log p y0 t z1 t u1 t c1 t 11.10 const log p x0 t log p xt xt 1 ut i log p zi t yt ci t
