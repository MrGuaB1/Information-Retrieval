ee362 final project index inferring depth from images introduction depth cues oculomotor cues binocular cues monocular cues implementations texture atmospheric perspective motion parallax absolute recovery conclusions references appendix files division of labor ee362 final project winter 2005 inferring depth from images david lieb andrew lookingbill keith rauenbuehler introduction as part of research being conducted through the stanford artificial intelligence laboratory our group looked at methods for inferring depth information from an image taken by a single camera this could be used as a replacement for traditional 3 d reconstruction approaches typically taken in computer vision the end goal was to implement code that could be run on the robot part of the darpa lagr project pictured below this work was intended to build on work previously done to segment a 2 d scene into traversable and non traversable regions given the output of an algorithm that correctly labeled trees as hazards in a monocular camera image blue and red regions in the picture below we were seeking to infer the depths of the different obstacles in the scene which would allow the robot to navigate around them in keeping with the spirit of this course we decided to approach this problem by examining the ways in which the human vision system tackles the problem of depth perception we examined the classes of depth cues discussed by e bruce goldstein in his book sensation and perception these along with any analogs in the computer vision field are discussed in the depth cues section we implemented several of these approaches in c and matlab to evaluate their performance and integrated elements from several of them into what turned out to be our most successful approach to monocular depth perception the results from this work are shown in the implementation section
